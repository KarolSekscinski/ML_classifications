{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:45:04.787568Z",
     "start_time": "2025-05-10T20:45:04.771115Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "file_name = \"../../data/raw/NeurIPS/Base.csv\"\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(file_name)\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: CSV file not found at the specified path. Please check the 'data_source' variable.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:45:08.428797Z",
     "start_time": "2025-05-10T20:45:04.790575Z"
    }
   },
   "id": "c1f64e19f9ff5ae1",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Delete column with all values the same\n",
      "\n",
      "Performing initial data cleaning (handling -1 as NaN)...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Initial Data Cleaning (from EDA) ---\n",
    "print(\"\\nDelete column with all values the same\")\n",
    "df = df.drop(columns=['device_fraud_count'])\n",
    "\n",
    "print(\"\\nPerforming initial data cleaning (handling -1 as NaN)...\")\n",
    "# Columns where -1 represents a missing value based on domain knowledge\n",
    "cols_with_minus_one_as_nan = [\n",
    "    'prev_address_months_count', \n",
    "    'current_address_months_count', \n",
    "    'bank_months_count',\n",
    "    'session_length_in_minutes'\n",
    "]\n",
    "for col in cols_with_minus_one_as_nan:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace(-1, np.nan)\n",
    "        # print(f\"Replaced -1 with NaN in '{col}'. Missing now: {df[col].isnull().sum()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:45:08.647838Z",
     "start_time": "2025-05-10T20:45:08.430799Z"
    }
   },
   "id": "333c8ea206992c1d",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining feature types...\n",
      "Target: fraud_bool\n",
      "Numerical features (17): ['income', 'name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'days_since_request', 'intended_balcon_amount', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', 'bank_months_count', 'proposed_credit_limit', 'session_length_in_minutes', 'device_distinct_emails_8w']\n",
      "Categorical features (7): ['customer_age', 'payment_type', 'employment_status', 'housing_status', 'source', 'device_os', 'month']\n",
      "Binary features (6): ['email_is_free', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'foreign_request', 'keep_alive_session']\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Define Feature Types (based on domain knowledge) ---\n",
    "print(\"\\nDefining feature types...\")\n",
    "target = 'fraud_bool'\n",
    "\n",
    "# Numerical features for imputation and scaling\n",
    "numerical_features = [\n",
    "    'income', 'name_email_similarity', 'prev_address_months_count', \n",
    "    'current_address_months_count', 'days_since_request', 'intended_balcon_amount',\n",
    "    'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', \n",
    "    'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score',\n",
    "    'bank_months_count', 'proposed_credit_limit', 'session_length_in_minutes',\n",
    "    'device_distinct_emails_8w'\n",
    "]\n",
    "# Filter out any columns not present in the loaded dataframe (e.g., if using a sample)\n",
    "numerical_features = [col for col in numerical_features if col in df.columns]\n",
    "\n",
    "# Categorical features for imputation and one-hot encoding\n",
    "# customer_age and month are treated as categorical due to their binned/discrete nature\n",
    "categorical_features = [\n",
    "    'customer_age', \n",
    "    'payment_type', \n",
    "    'employment_status', \n",
    "    'housing_status',\n",
    "    'source', \n",
    "    'device_os',\n",
    "    'month'\n",
    "]\n",
    "categorical_features = [col for col in categorical_features if col in df.columns]\n",
    "\n",
    "# Binary features (already 0/1) - will be passed through or scaled if needed later\n",
    "# For this stage, we'll ensure they are not mistakenly one-hot encoded if they are part of X.\n",
    "# They don't typically need imputation if they are clean 0/1.\n",
    "binary_features = [\n",
    "    'email_is_free', 'phone_home_valid', 'phone_mobile_valid', \n",
    "    'has_other_cards', 'foreign_request', 'keep_alive_session'\n",
    "]\n",
    "binary_features = [col for col in binary_features if col in df.columns]\n",
    "\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"Binary features ({len(binary_features)}): {binary_features}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:45:08.663839Z",
     "start_time": "2025-05-10T20:45:08.649838Z"
    }
   },
   "id": "66e03dcd9750f172",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X: (1000000, 30), Shape of y: (1000000,)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Separate Features (X) and Target (y) ---\n",
    "if target not in df.columns:\n",
    "    print(f\"ERROR: Target column '{target}' not found in the DataFrame.\")\n",
    "    exit()\n",
    "\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "print(f\"\\nShape of X: {X.shape}, Shape of y: {y.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:45:08.867478Z",
     "start_time": "2025-05-10T20:45:08.665838Z"
    }
   },
   "id": "35fb0a50cb23f23b",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up preprocessing pipelines...\n",
      "Preprocessor configured.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Preprocessing Pipelines ---\n",
    "print(\"\\nSetting up preprocessing pipelines...\")\n",
    "\n",
    "# Pipeline for numerical features: Median Imputation + Standard Scaling\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_median', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()) # Important for SVM, good for XGBoost too\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features: Mode Imputation + One-Hot Encoding\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_mode', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # sparse_output=False for easier handling with SMOTE\n",
    "])\n",
    "\n",
    "# Create ColumnTransformer to apply pipelines to correct columns\n",
    "# Binary features are currently passed through ('passthrough').\n",
    "# If scaling is desired for binary features as well, they can be added to numerical_transformer.\n",
    "# However, scaling 0/1 features usually doesn't change their meaning much unless they have NaNs.\n",
    "# We ensure binary features are not in numerical or categorical lists to avoid double processing.\n",
    "X_numerical = [col for col in numerical_features if col in X.columns]\n",
    "X_categorical = [col for col in categorical_features if col in X.columns]\n",
    "X_binary_passthrough = [col for col in binary_features if col in X.columns and col not in X_numerical and col not in X_categorical]\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, X_numerical),\n",
    "        ('cat', categorical_pipeline, X_categorical)\n",
    "        # ('bin', 'passthrough', X_binary_passthrough) # Option to passthrough binary\n",
    "    ], \n",
    "    remainder='passthrough' # Keep other columns (like binary ones not explicitly handled if any)\n",
    "                            # or 'drop' if you only want processed columns\n",
    ")\n",
    "print(\"Preprocessor configured.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:45:24.977686Z",
     "start_time": "2025-05-10T20:45:24.958689Z"
    }
   },
   "id": "87a95df82e2be4fb",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data into training and testing sets (80/20 split)...\n",
      "X_train shape: (800000, 30), y_train shape: (800000,)\n",
      "X_test shape: (200000, 30), y_test shape: (200000,)\n",
      "Fraud distribution in original y: \n",
      "fraud_bool\n",
      "0    0.988971\n",
      "1    0.011029\n",
      "Name: proportion, dtype: float64\n",
      "Fraud distribution in y_train: \n",
      "fraud_bool\n",
      "0    0.988971\n",
      "1    0.011029\n",
      "Name: proportion, dtype: float64\n",
      "Fraud distribution in y_test: \n",
      "fraud_bool\n",
      "0    0.98897\n",
      "1    0.01103\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Split Data into Training and Testing sets ---\n",
    "print(\"\\nSplitting data into training and testing sets (80/20 split)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y # Crucial for imbalanced datasets\n",
    ")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "print(f\"Fraud distribution in original y: \\n{y.value_counts(normalize=True)}\")\n",
    "print(f\"Fraud distribution in y_train: \\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Fraud distribution in y_test: \\n{y_test.value_counts(normalize=True)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:45:41.262819Z",
     "start_time": "2025-05-10T20:45:40.412119Z"
    }
   },
   "id": "606163a0d4878ccf",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying preprocessing (fitting on train, transforming train and test)...\n",
      "Processed training and testing data (X) are now NumPy arrays.\n",
      "Shape of X_train_processed: (800000, 66)\n",
      "Shape of X_test_processed: (200000, 66)\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Apply Preprocessing ---\n",
    "# Fit the preprocessor on the training data and transform both training and testing data\n",
    "print(\"\\nApplying preprocessing (fitting on train, transforming train and test)...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names after one-hot encoding for creating DataFrames (optional, but good for inspection)\n",
    "try:\n",
    "    # Get feature names from the 'cat' part of the preprocessor\n",
    "    ohe_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(X_categorical)\n",
    "    \n",
    "    # Combine with numerical and passthrough columns\n",
    "    # Order matters: numerical, then OHE categorical, then remainder\n",
    "    processed_feature_names = X_numerical + list(ohe_feature_names)\n",
    "    \n",
    "    # Handling 'remainder' columns:\n",
    "    # The 'remainder' columns are those not in X_numerical or X_categorical.\n",
    "    # We need to get their names correctly based on the ColumnTransformer's behavior.\n",
    "    # This part can be tricky if 'remainder' columns are not at the end or if their order changes.\n",
    "    # A simpler way if you know the remainder columns:\n",
    "    remainder_cols = [col for col in X_train.columns if col not in X_numerical and col not in X_categorical]\n",
    "    processed_feature_names.extend(remainder_cols)\n",
    "\n",
    "    # Convert processed arrays back to DataFrames (optional, for inspection)\n",
    "    X_train_processed_df = pd.DataFrame(X_train_processed, columns=processed_feature_names, index=X_train.index)\n",
    "    X_test_processed_df = pd.DataFrame(X_test_processed, columns=processed_feature_names, index=X_test.index)\n",
    "    print(\"Processed training and testing data (X) are now NumPy arrays.\")\n",
    "    print(f\"Shape of X_train_processed: {X_train_processed.shape}\")\n",
    "    print(f\"Shape of X_test_processed: {X_test_processed.shape}\")\n",
    "    # print(\"First 5 rows of X_train_processed_df (for inspection):\")\n",
    "    # print(X_train_processed_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Could not reconstruct feature names after OHE: {e}\")\n",
    "    print(\"Proceeding with NumPy arrays for X_train_processed and X_test_processed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:46:10.157694Z",
     "start_time": "2025-05-10T20:46:04.943703Z"
    }
   },
   "id": "3d61d0575ad022e2",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['income', 'name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'days_since_request', 'intended_balcon_amount', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', 'bank_months_count', 'proposed_credit_limit', 'session_length_in_minutes', 'device_distinct_emails_8w', 'customer_age_10', 'customer_age_20', 'customer_age_30', 'customer_age_40', 'customer_age_50', 'customer_age_60', 'customer_age_70', 'customer_age_80', 'customer_age_90', 'payment_type_AA', 'payment_type_AB', 'payment_type_AC', 'payment_type_AD', 'payment_type_AE', 'employment_status_CA', 'employment_status_CB', 'employment_status_CC', 'employment_status_CD', 'employment_status_CE', 'employment_status_CF', 'employment_status_CG', 'housing_status_BA', 'housing_status_BB', 'housing_status_BC', 'housing_status_BD', 'housing_status_BE', 'housing_status_BF', 'housing_status_BG', 'source_INTERNET', 'source_TELEAPP', 'device_os_linux', 'device_os_macintosh', 'device_os_other', 'device_os_windows', 'device_os_x11', 'month_0', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'email_is_free', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'foreign_request', 'keep_alive_session']\n"
     ]
    }
   ],
   "source": [
    "print(processed_feature_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T22:33:27.261282Z",
     "start_time": "2025-05-10T22:33:27.251262Z"
    }
   },
   "id": "790a96c3b15462b",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying SMOTE to the training data to handle class imbalance...\n",
      "SMOTE applied successfully.\n",
      "Shape of X_train_resampled: (1582354, 66)\n",
      "Shape of y_train_resampled: (1582354,)\n",
      "Fraud distribution in y_train_resampled: \n",
      "fraud_bool\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Resampling the Training Data using SMOTE ---\n",
    "# SMOTE should only be applied to the training data\n",
    "print(\"\\nApplying SMOTE to the training data to handle class imbalance...\")\n",
    "# Check if there are enough samples in the minority class for SMOTE\n",
    "# SMOTE's k_neighbors default is 5. It needs at least k_neighbors + 1 samples in the minority class.\n",
    "minority_class_count = y_train.value_counts().min()\n",
    "k_neighbors_smote = min(5, minority_class_count - 1) if minority_class_count > 1 else 1\n",
    "\n",
    "if k_neighbors_smote < 1 and minority_class_count <=1 : # If minority class has 0 or 1 sample\n",
    "    print(f\"WARNING: Minority class in training set has only {minority_class_count} sample(s).\")\n",
    "    print(\"SMOTE cannot be applied effectively or may fail. Consider alternative strategies or more data.\")\n",
    "    print(\"Proceeding without SMOTE for now.\")\n",
    "    X_train_resampled = X_train_processed\n",
    "    y_train_resampled = y_train\n",
    "else:\n",
    "    smote = SMOTE(random_state=42, k_neighbors=k_neighbors_smote)\n",
    "    try:\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "        print(\"SMOTE applied successfully.\")\n",
    "        print(f\"Shape of X_train_resampled: {X_train_resampled.shape}\")\n",
    "        print(f\"Shape of y_train_resampled: {y_train_resampled.shape}\")\n",
    "        print(f\"Fraud distribution in y_train_resampled: \\n{pd.Series(y_train_resampled).value_counts(normalize=True)}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during SMOTE: {e}\")\n",
    "        print(\"This might happen if the minority class count is too low even after adjusting k_neighbors.\")\n",
    "        print(\"Proceeding with original (imbalanced) training data for now.\")\n",
    "        X_train_resampled = X_train_processed\n",
    "        y_train_resampled = y_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:47:06.298879Z",
     "start_time": "2025-05-10T20:47:01.675284Z"
    }
   },
   "id": "ed73cbdf3cb89f81",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_saving_place = \"../../data/processed/NeurIPS\"\n",
    "\n",
    "# X_train_resampled = pd.DataFrame(X_train_resampled, columns=processed_feature_names, index=X_train.index)\n",
    "# y_train_resampled = pd.DataFrame(y_train_resampled, columns=processed_feature_names, index=y_train.index)\n",
    "# X_train_resampled.to_csv(f\"{base_saving_place}/X_train_resampled.csv\")\n",
    "# y_train_resampled.to_csv(f\"{base_saving_place}/y_train_resampled.csv\")\n",
    "\n",
    "X_train_processed_df.to_csv(f\"{base_saving_place}/X_train_without_resampling.csv\", index=False)\n",
    "X_test_processed_df.to_csv(f\"{base_saving_place}/X_test_without_resampling.csv\", index=False)\n",
    "y_train.to_csv(f\"{base_saving_place}/y_train.csv\", index=False)\n",
    "y_test.to_csv(f\"{base_saving_place}/y_test.csv\", index=False)\n",
    "X_train.to_csv(f\"{base_saving_place}/X_train.csv\", index=False)\n",
    "X_test.to_csv(f\"{base_saving_place}/X_test.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T22:30:26.273286Z",
     "start_time": "2025-05-10T22:29:27.019012Z"
    }
   },
   "id": "c73169c950a2ad8c",
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
